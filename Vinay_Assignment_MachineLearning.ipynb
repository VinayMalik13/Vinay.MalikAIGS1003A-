{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To find : P (C sunny/ a cone of ice-cream) = ? <br>\n",
    "P (rainy /a cup of hot coffee)=? <br>\n",
    "Given the occurrence of each word is Independent<br>\n",
    "P (a cone of ice cream) = P(a) (cone) P(of) P(ice) P(cream)** <br>\n",
    "**Part a)** <br>\n",
    "P(Sunny/a cone of ice cream) <br>\n",
    "_Using Bayes theorem :P(A/B) = (PB/A)*P(A)/P(B)_ <br>\n",
    "we can find <br>\n",
    "P (Sunny/a cone of Ice-cream) = P (a cone of Ice-cream/sunny) * (P (sunny)/ P(a cone of Ice-cream)) \n",
    "Also, since the words are independent: <br>\n",
    "**we want to classify the tags with higher probability, so ignoring the denominator** <br>\n",
    "therefore: - <br>\n",
    "P (Sunny/a cone of Ice-cream) = P(a/sunny) * P (Cone/sunny) * P (of/ sunny) * P(Ice/ sunny)* P (Cream/sunny)* P (Sunny)<br>\n",
    "\n",
    "**Part B)** <br>\n",
    "Similarly: -<br>\n",
    "P (rainy/a cup of hot coffee) = P(a/rainy).P(cup/rainy).P(of/rainy).P(hot/rainy).P(coffee/rainy).P(rainy) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the Implementation trainAndTune and calculateLogJointProbabilities in the given naiveBayes.py\n",
    "\n",
    "please find the file below.\n",
    "\n",
    "Referenced: https://github.com/anthony-niklas/cs188/blob/master/p5/naiveBayes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# naiveBayes.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensing Information: Please do not distribute or publish solutions to this\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclassificationMethod\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "# naiveBayes.py\n",
    "# -------------\n",
    "# Licensing Information: Please do not distribute or publish solutions to this\n",
    "# project. You are free to use and extend these projects for educational\n",
    "# purposes. The Pacman AI projects were developed at UC Berkeley, primarily by\n",
    "# John DeNero (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# For more info, see http://inst.eecs.berkeley.edu/~cs188/sp09/pacman.html\n",
    "\n",
    "import util\n",
    "import classificationMethod\n",
    "import math\n",
    "\n",
    "class NaiveBayesClassifier(classificationMethod.ClassificationMethod):\n",
    "    \"\"\"\n",
    "    See the project description for the specifications of the Naive Bayes classifier.\n",
    "    \n",
    "    Note that the variable 'datum' in this code refers to a counter of features\n",
    "    (not to a raw samples.Datum).\n",
    "    \"\"\"\n",
    "    def __init__(self, legalLabels):\n",
    "        self.legalLabels = legalLabels\n",
    "        self.type = \"naivebayes\"\n",
    "        self.k = 1 # this is the smoothing parameter, ** use it in your train method **\n",
    "        self.automaticTuning = False # Look at this flag to decide whether to choose k automatically ** use this in your train method **\n",
    "        \n",
    "    def setSmoothing(self, k):\n",
    "        \"\"\"\n",
    "        This is used by the main method to change the smoothing parameter before training.\n",
    "        Do not modify this method.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def train(self, trainingData, trainingLabels, validationData, validationLabels):\n",
    "        \"\"\"\n",
    "        Outside shell to call your method. Do not modify this method.\n",
    "        \"\"\"    \n",
    "            \n",
    "        # might be useful in your code later...\n",
    "        # this is a list of all features in the training set.\n",
    "        self.features = list(set([ f for datum in trainingData for f in datum.keys() ]));\n",
    "        \n",
    "        if (self.automaticTuning):\n",
    "                kgrid = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 20, 50]\n",
    "        else:\n",
    "                kgrid = [self.k]\n",
    "                \n",
    "        self.trainAndTune(trainingData, trainingLabels, validationData, validationLabels, kgrid)\n",
    "            \n",
    "    def trainAndTune(self, trainingData, trainingLabels, validationData, validationLabels, kgrid):\n",
    "        \"\"\"\n",
    "        Trains the classifier by collecting counts over the training data, and\n",
    "        stores the Laplace smoothed estimates so that they can be used to classify.\n",
    "        Evaluate each value of k in kgrid to choose the smoothing parameter \n",
    "        that gives the best accuracy on the held-out validationData.\n",
    "        \n",
    "        trainingData and validationData are lists of feature Counters.    The corresponding\n",
    "        label lists contain the correct label for each datum.\n",
    "        \n",
    "        To get the list of all possible features or labels, use self.features and \n",
    "        self.legalLabels.\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        P = util.Counter()\n",
    "        for l in trainingLabels:\n",
    "            P[l] += 1\n",
    "        P.normalize()\n",
    "        self.P = P\n",
    "        \n",
    "        # Initialize stuff\n",
    "        counts = {}\n",
    "        totals = {}\n",
    "        for f in self.features:\n",
    "            counts[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "            totals[f] = util.Counter()\n",
    "                     \n",
    "        # Calculate totals and counts\n",
    "        for i, datum in enumerate(trainingData):\n",
    "            y = trainingLabels[i]\n",
    "            for f, value in datum.items():\n",
    "                counts[f][value][y] += 1.0\n",
    "                totals[f][y] += 1.0 \n",
    "                \n",
    "        bestConditionals = {}\n",
    "        bestAccuracy = None\n",
    "        # Evaluate each k, and use the one that yields the best accuracy\n",
    "        for k in kgrid or [0.0]:\n",
    "            correct = 0\n",
    "            conditionals = {}            \n",
    "            for f in self.features:\n",
    "                conditionals[f] = {0: util.Counter(), 1: util.Counter()}\n",
    "                \n",
    "            # Run Laplace smoothing\n",
    "            for f in self.features:\n",
    "                for value in [0, 1]:\n",
    "                    for y in self.legalLabels:\n",
    "                        conditionals[f][value][y] = (counts[f][value][y] + k) / (totals[f][y] + k * 2)\n",
    "                \n",
    "            # Check the accuracy associated with this k\n",
    "            self.conditionals = conditionals              \n",
    "            guesses = self.classify(validationData)\n",
    "            for i, guess in enumerate(guesses):\n",
    "                correct += (validationLabels[i] == guess and 1.0 or 0.0)\n",
    "            accuracy = correct / len(guesses)\n",
    "            \n",
    "            # Keep the best k so far\n",
    "            if accuracy > bestAccuracy or bestAccuracy is None:\n",
    "                bestAccuracy = accuracy\n",
    "                bestConditionals = conditionals\n",
    "                self.k = k\n",
    "                \n",
    "        self.conditionals = bestConditionals\n",
    "                \n",
    "    def classify(self, testData):\n",
    "        \"\"\"\n",
    "        Classify the data based on the posterior distribution over labels.\n",
    "        \n",
    "        You shouldn't modify this method.\n",
    "        \"\"\"\n",
    "        guesses = []\n",
    "        self.posteriors = [] # Log posteriors are stored for later data analysis (autograder).\n",
    "        for datum in testData:\n",
    "            posterior = self.calculateLogJointProbabilities(datum)\n",
    "            guesses.append(posterior.argMax())\n",
    "            self.posteriors.append(posterior)\n",
    "        return guesses\n",
    "            \n",
    "    def calculateLogJointProbabilities(self, datum):\n",
    "        \"\"\"\n",
    "        Returns the log-joint distribution over legal labels and the datum.\n",
    "        Each log-probability should be stored in the log-joint counter, e.g.        \n",
    "        logJoint[3] = <Estimate of log( P(Label = 3, datum) )>\n",
    "        \n",
    "        To get the list of all possible features or labels, use self.features and \n",
    "        self.legalLabels.\n",
    "        \"\"\"\n",
    "        logJoint = util.Counter()\n",
    "        evidence = datum.items()\n",
    "        \"*** YOUR CODE HERE ***\"\n",
    "        for y in self.legalLabels:\n",
    "            logJoint[y] = math.log(self.P[y])\n",
    "            for f in self.conditionals:\n",
    "                prob = self.conditionals[f][datum[f]][y]\n",
    "                logJoint[y] += (prob and math.log(prob) or 0.0)\n",
    "\n",
    "        return logJoint\n",
    "    \n",
    "    def findHighOddsFeatures(self, label1, label2):\n",
    "        \"\"\"\n",
    "        Returns the 100 best features for the odds ratio:\n",
    "                        P(feature = 1 | label1) / P(feature = 1 | label2) \n",
    "        \n",
    "        Note: you may find 'self.features' a useful way to loop through all possible features\n",
    "        \"\"\"\n",
    "        \"*** YOUR CODE HERE ***\"        \n",
    "        featuresOdds = []\n",
    "        for f in self.features:\n",
    "            top = self.conditionals[f][1][label1]\n",
    "            bottom = self.conditionals[f][1][label2]\n",
    "            ratio = top / bottom\n",
    "            featuresOdds.append((f, ratio))\n",
    "            \n",
    "        featuresOdds = [f for f, odds in sorted(featuresOdds, key=lambda t: -t[1])[:100]]\n",
    "\n",
    "        return featuresOdds\n",
    "        \n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "Doing classification\n",
    "--------------------\n",
    "data:           digits\n",
    "classifier:             naiveBayes\n",
    "training set size:      100\n",
    "Extracting features...\n",
    "Training...\n",
    "Validating...\n",
    "72 correct out of 100 (72.0%).\n",
    "Testing...\n",
    "62 correct out of 100 (62.0%).\n",
    "===================================\n",
    "Mistake on example 2\n",
    "Predicted 1; truth is 2\n",
    "Image:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           +++#####+\n",
    "          +#########+\n",
    "          +##########+\n",
    "           ++++  +###+\n",
    "                  +##+\n",
    "                 +###+\n",
    "                +###+\n",
    "               +###+\n",
    "              +####+\n",
    "              +###+\n",
    "             +###+\n",
    "            +###+\n",
    "           +###+\n",
    "          +###+\n",
    "         +####+\n",
    "        +####+  ++++\n",
    "       +#####++###++\n",
    "      +##########+\n",
    "       +#######+\n",
    "         ++++"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
